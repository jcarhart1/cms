{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 2604720,
     "sourceType": "datasetVersion",
     "datasetId": 1583027
    },
    {
     "sourceId": 2701830,
     "sourceType": "datasetVersion",
     "datasetId": 1645661
    },
    {
     "sourceId": 2714421,
     "sourceType": "datasetVersion",
     "datasetId": 1653824
    }
   ],
   "dockerImageVersionId": 30138,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CareHarmony Case Study\n",
    "\n",
    "You will be using the CMS 2008-2010 Data Entrepreneurs Synthetic PUF beneficiary data files for this project. This dataset is a bit old, but it is a large synthetic dataset that closely mirrors the type of data that we work with. Due to file size limitations, each data type in the CMS Linkable 2008-2010 Medicare DE-SynPUF is released in 20 separate samples (essentially each is a .25% sample)\n",
    "\n",
    "The DE-SynPUF contains five types of data for the period 2008-2010:\n",
    "* Beneficiary Summary\n",
    "* Inpatient Claims\n",
    "* Outpatient Claims\n",
    "* Carrier Claims (not included in the current analysis)\n",
    "* Prescription Drug Events (not included in the current analysis)\n",
    "\n",
    "\n",
    "To keep things simple, you will be primarily utilizing the beneficiary-level files that can be found in the zip file attached. These data files contain high-level summaries of patient demographics, conditions, and spending for each of the years 2008, 2009, and 2010. It is organized into twenty separate files for each year and organized such that the same patients are retained in each of the yearly files sharing the same index number. Ten files per year are included for completeness, but you may use as many as you like to conduct your analysis. Note that the dataset is indeed synthetic so many relationships we would expect to see in real world data may not hold. Accordingly, in our evaluation we will focus more on your analytical approach than the actual results.\n",
    "\n",
    "The original source of the files can be found at this link: https://www.cms.gov/Research-Statistics-Data-and-Systems/Downloadable-Public-Use-Files/SynPUFs/DESample01. Translations for a lot of the common terms provided in the Beneficiary Summary file can be found at this link: https://www.cms.gov/files/document/de-10-codebook.pdf-0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CareHarmony Case Study: causal modeling\n",
    "\n",
    "The health system is also considering deploying a newly developed intervention that reduces the likelihood of developing congestive heart failure (CHF) without impacting any other factors of a patient's health. Before deploying the intervention, they would like to estimate its impact in reducing costs due to the new development of CHF. To help answer this question, please use the data to estimate the impact of a patient being developing diagnosis-level CHF on their total cost in the following year."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import sns as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Beneficiary data processing**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Define the base directory for our CSV files\n",
    "base_dir = r'C:\\code_personal_use\\cms\\data\\bene'\n",
    "\n",
    "# List of years we have data for\n",
    "years = [2008, 2009, 2010]\n",
    "\n",
    "# Columns to use for each year\n",
    "columns_2008 = [\n",
    "    'DESYNPUF_ID', 'BENE_BIRTH_DT', 'BENE_SEX_IDENT_CD', 'BENE_RACE_CD',\n",
    "    'SP_STATE_CODE', 'BENE_COUNTY_CD', 'SP_ALZHDMTA', 'SP_CHF', 'SP_CHRNKIDN',\n",
    "    'SP_CNCR', 'BENE_ESRD_IND', 'SP_COPD', 'SP_DEPRESSN', 'SP_DIABETES',\n",
    "    'SP_ISCHMCHT', 'SP_OSTEOPRS', 'SP_RA_OA', 'SP_STRKETIA', 'MEDREIMB_IP',\n",
    "    'BENRES_IP', 'PPPYMT_IP', 'MEDREIMB_OP', 'BENRES_OP', 'PPPYMT_OP',\n",
    "    'MEDREIMB_CAR', 'BENRES_CAR', 'PPPYMT_CAR'\n",
    "]\n",
    "\n",
    "columns_2009_2010 = [\n",
    "    'DESYNPUF_ID', 'SP_ALZHDMTA', 'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR',\n",
    "    'BENE_ESRD_IND', 'SP_COPD', 'SP_DEPRESSN', 'SP_DIABETES', 'SP_ISCHMCHT',\n",
    "    'SP_OSTEOPRS', 'SP_RA_OA', 'SP_STRKETIA', 'MEDREIMB_IP', 'BENRES_IP',\n",
    "    'PPPYMT_IP', 'MEDREIMB_OP', 'BENRES_OP', 'PPPYMT_OP', 'MEDREIMB_CAR',\n",
    "    'BENRES_CAR', 'PPPYMT_CAR'\n",
    "]\n",
    "\n",
    "# Function to recode BENE_ESRD_IND\n",
    "def recode_bene_esrd_ind(df, column):\n",
    "    df[column] = df[column].replace({'Y': 1, '0': 2}).astype(int)\n",
    "    return df\n",
    "\n",
    "# Function to rename columns with a suffix\n",
    "def rename_columns_with_suffix(df, suffix):\n",
    "    df = df.rename(columns=lambda x: x + suffix if x != 'DESYNPUF_ID' else x)\n",
    "    return df\n",
    "\n",
    "# Function to load and preprocess a batch of files for a given year\n",
    "def load_and_preprocess_batch(file_paths, year):\n",
    "    columns = columns_2008 if year == 2008 else columns_2009_2010\n",
    "    parse_dates = ['BENE_BIRTH_DT'] if year == 2008 else None\n",
    "    date_parser = lambda x: pd.to_datetime(x, format='%Y%m%d') if year == 2008 else None\n",
    "\n",
    "    # Load and preprocess files in the batch\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            usecols=columns,\n",
    "            parse_dates=parse_dates,\n",
    "            date_parser=date_parser\n",
    "        )\n",
    "        df = recode_bene_esrd_ind(df, 'BENE_ESRD_IND')\n",
    "        df = rename_columns_with_suffix(df, f'_{year}')\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the batch\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Create a dictionary to store DataFrames for each year\n",
    "dfs_by_year = {}\n",
    "\n",
    "# Process files in batches\n",
    "batch_size = 5  # Adjust the batch size based on your system's memory\n",
    "for year in years:\n",
    "    file_paths = [os.path.join(base_dir, f'DE1_0_{year}_Beneficiary_Summary_File_Sample_{i}.csv') for i in range(1, 20)]\n",
    "    batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
    "\n",
    "    # Load and preprocess each batch\n",
    "    year_dfs = []\n",
    "    for batch in batches:\n",
    "        batch_df = load_and_preprocess_batch(batch, year)\n",
    "        year_dfs.append(batch_df)\n",
    "\n",
    "    # Concatenate all batches for the year\n",
    "    dfs_by_year[year] = pd.concat(year_dfs, ignore_index=True)\n",
    "\n",
    "# Merge DataFrames for all years on DESYNPUF_ID\n",
    "merged_df = dfs_by_year[2008]\n",
    "for year in years[1:]:\n",
    "    merged_df = merged_df.merge(dfs_by_year[year], on='DESYNPUF_ID', how='inner')\n",
    "\n",
    "# Define the prefixes to check for null values\n",
    "prefixes = [\n",
    "    'MEDREIMB_IP', 'BENRES_IP', 'PPPYMT_IP', 'MEDREIMB_OP', 'BENRES_OP',\n",
    "    'PPPYMT_OP', 'MEDREIMB_CAR', 'BENRES_CAR', 'PPPYMT_CAR'\n",
    "]\n",
    "\n",
    "# Function to replace null values with 0 for specified prefixes\n",
    "def replace_nulls_with_zero(df, prefixes):\n",
    "    for prefix in prefixes:\n",
    "        cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "        df[cols] = df[cols].fillna(0)\n",
    "    return df\n",
    "\n",
    "merged_df = replace_nulls_with_zero(merged_df, prefixes)\n",
    "\n",
    "# Calculate Age\n",
    "def calculate_age(birth_date, reference_date):\n",
    "    return reference_date.year - birth_date.year - ((reference_date.month, reference_date.day) < (birth_date.month, birth_date.day))\n",
    "\n",
    "reference_date = datetime.now()  # Using the current date as the reference date\n",
    "merged_df['Age'] = merged_df['BENE_BIRTH_DT_2008'].apply(lambda x: calculate_age(x, reference_date))\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame and unique ID counts\n",
    "print(f'Unique IDs in 2008: {dfs_by_year[2008][\"DESYNPUF_ID\"].nunique()}')\n",
    "print(f'Unique IDs in 2009: {dfs_by_year[2009][\"DESYNPUF_ID\"].nunique()}')\n",
    "print(f'Unique IDs in 2010: {dfs_by_year[2010][\"DESYNPUF_ID\"].nunique()}')\n",
    "print(f'Unique IDs after merging: {merged_df[\"DESYNPUF_ID\"].nunique()}')\n",
    "\n",
    "print(\"First few rows of the merged DataFrame:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "merged_df.info()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2021-10-18T04:32:09.367649Z",
     "iopub.execute_input": "2021-10-18T04:32:09.368574Z",
     "iopub.status.idle": "2021-10-18T04:32:09.473489Z",
     "shell.execute_reply.started": "2021-10-18T04:32:09.368526Z",
     "shell.execute_reply": "2021-10-18T04:32:09.47243Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df.describe(())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Need to work on optimizing this step. It currently takes a little over 5 mins of run time to finish merging all 20 beneficiary files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Filter to include only the specific DESYNPUF_ID for EDA and QA\n",
    "# merged_df = merged_df[merged_df['DESYNPUF_ID'] == '00016F745862898F']\n",
    "\n",
    "# List of columns to sum across years\n",
    "columns_to_sum = [\n",
    "    'MEDREIMB_IP', 'BENRES_IP', 'PPPYMT_IP',\n",
    "    'MEDREIMB_OP', 'BENRES_OP', 'PPPYMT_OP',\n",
    "    'MEDREIMB_CAR', 'BENRES_CAR', 'PPPYMT_CAR'\n",
    "]\n",
    "\n",
    "# Loop through each column and create a new column for the sum across the three years\n",
    "for col in columns_to_sum:\n",
    "    merged_df[col + '_SUM'] = merged_df[col + '_2008'] + merged_df[col + '_2009'] + merged_df[col + '_2010']\n",
    "\n",
    "# Create a list of columns that end with '_SUM'\n",
    "sum_columns = [col + '_SUM' for col in columns_to_sum]\n",
    "\n",
    "# Add a new column 'total_sum' that sums all columns ending with '_SUM'\n",
    "merged_df['total_sum'] = merged_df[sum_columns].sum(axis=1)\n",
    "\n",
    "# Extract additional columns from the 2008 dataset to serve as our demographic features\n",
    "additional_columns_08 = merged_df[['DESYNPUF_ID', 'BENE_BIRTH_DT_2008', 'Age', 'BENE_SEX_IDENT_CD_2008', 'BENE_RACE_CD_2008', 'SP_STATE_CODE_2008', 'BENE_COUNTY_CD_2008']]\n",
    "# Rename columns to more human-readable names\n",
    "additional_columns_08 = additional_columns_08.rename(columns={\n",
    "    'BENE_BIRTH_DT_2008': 'Birth_Date',\n",
    "    'BENE_SEX_IDENT_CD_2008': 'Gender',\n",
    "    'BENE_RACE_CD_2008': 'Race',\n",
    "    'SP_STATE_CODE_2008': 'State',\n",
    "    'BENE_COUNTY_CD_2008': 'County'\n",
    "})\n",
    "\n",
    "# Export the wide format DataFrame to a CSV file for QA\n",
    "# output_path_wide = r'C:\\Users\\JoeCarhart\\OneDrive - Appriss Health LLC\\Desktop\\merged_data_wide.csv'\n",
    "# merged_df.to_csv(output_path_wide, index=False)\n",
    "\n",
    "# Reshape the DataFrame from wide to long format for the specified columns\n",
    "columns_to_reshape = ['DESYNPUF_ID'] + [col + suffix for col in columns_to_sum for suffix in ['_2008', '_2009', '_2010']]\n",
    "df_wide = merged_df[columns_to_reshape]\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame to long format\n",
    "df_long = pd.melt(df_wide, id_vars=['DESYNPUF_ID'],\n",
    "                  value_vars=[col + suffix for col in columns_to_sum for suffix in ['_2008', '_2009', '_2010']],\n",
    "                  var_name='Year_Column', value_name='Value')\n",
    "\n",
    "# Split the 'Year_Column' to extract the year information and the original column name\n",
    "df_long['Year'] = df_long['Year_Column'].str.extract(r'(\\d{4})$')\n",
    "df_long['Column'] = df_long['Year_Column'].str.replace(r'_\\d{4}$', '')\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "df_long['Year'] = pd.to_numeric(df_long['Year'], errors='coerce')\n",
    "\n",
    "# Sum 'Value' by 'Year' and 'DESYNPUF_ID' to create 'annual_cost'\n",
    "annual_cost = df_long.groupby(['DESYNPUF_ID', 'Year'])['Value'].sum().reset_index()\n",
    "annual_cost.rename(columns={'Value': 'annual_cost'}, inplace=True)\n",
    "\n",
    "# Finalize the long format DataFrame to include DESYNPUF_ID, Year, and annual_cost\n",
    "df_long_final = annual_cost\n",
    "\n",
    "# Merge additional columns from 2008 dataset\n",
    "df_long_final = df_long_final.merge(additional_columns_08, on='DESYNPUF_ID', how='left')\n",
    "\n",
    "# Export the long format DataFrame to a CSV file for QA\n",
    "output_path_long = r'C:\\Users\\JoeCarhart\\OneDrive - Appriss Health LLC\\Desktop\\merged_data_long.csv'\n",
    "df_long_final.to_csv(output_path_long, index=False)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrames\n",
    "print(\"Wide format DataFrame:\")\n",
    "print(merged_df.head())\n",
    "print(\"\\nLong format DataFrame:\")\n",
    "print(df_long_final.head())\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_long_final.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code block needs furhter optimization. It currently takes close to 15 mins to process the diagnosis data for all 20 samples of beneficiary data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# merged_df_qa = merged_df[merged_df['DESYNPUF_ID'] == '00048EF1F4791C68']\n",
    "\n",
    "# Define the diagnosis columns and prefixes\n",
    "diagnosis_columns = [\n",
    "    'DESYNPUF_ID',\n",
    "    'SP_ALZHDMTA_2008', 'SP_CHF_2008', 'SP_CHRNKIDN_2008', 'SP_CNCR_2008', 'SP_COPD_2008',\n",
    "    'SP_DEPRESSN_2008', 'SP_DIABETES_2008', 'SP_ISCHMCHT_2008', 'SP_OSTEOPRS_2008',\n",
    "    'SP_RA_OA_2008', 'SP_STRKETIA_2008', 'BENE_ESRD_IND_2008',\n",
    "    'SP_ALZHDMTA_2009', 'SP_CHF_2009', 'SP_CHRNKIDN_2009', 'SP_CNCR_2009', 'SP_COPD_2009',\n",
    "    'SP_DEPRESSN_2009', 'SP_DIABETES_2009', 'SP_ISCHMCHT_2009', 'SP_OSTEOPRS_2009',\n",
    "    'SP_RA_OA_2009', 'SP_STRKETIA_2009', 'BENE_ESRD_IND_2009',\n",
    "    'SP_ALZHDMTA_2010', 'SP_CHF_2010', 'SP_CHRNKIDN_2010', 'SP_CNCR_2010', 'SP_COPD_2010',\n",
    "    'SP_DEPRESSN_2010', 'SP_DIABETES_2010', 'SP_ISCHMCHT_2010', 'SP_OSTEOPRS_2010',\n",
    "    'SP_RA_OA_2010', 'SP_STRKETIA_2010', 'BENE_ESRD_IND_2010'\n",
    "]\n",
    "\n",
    "# Create the diagnoses DataFrame from the main DataFrame\n",
    "diagnoses = merged_df[diagnosis_columns]\n",
    "\n",
    "# List of diagnosis prefixes\n",
    "diagnosis_prefixes = [\n",
    "    'SP_ALZHDMTA', 'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD',\n",
    "    'SP_DEPRESSN', 'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS',\n",
    "    'SP_RA_OA', 'SP_STRKETIA', 'BENE_ESRD_IND'\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store the long format data\n",
    "diagnoses_long_list = []\n",
    "\n",
    "# Transform each set of columns to long format and append them to the list\n",
    "for prefix in diagnosis_prefixes:\n",
    "    columns = [f'{prefix}_2008', f'{prefix}_2009', f'{prefix}_2010']\n",
    "    df_long = pd.melt(diagnoses, id_vars=['DESYNPUF_ID'], value_vars=columns,\n",
    "                      var_name='Year', value_name=prefix)\n",
    "    df_long['Year'] = df_long['Year'].str.extract(r'(\\d{4})$').astype(int)\n",
    "    diagnoses_long_list.append(df_long)\n",
    "\n",
    "# Combine all long format DataFrames\n",
    "diagnoses_long = diagnoses_long_list[0]\n",
    "for df_long in diagnoses_long_list[1:]:\n",
    "    diagnoses_long = diagnoses_long.merge(df_long, on=['DESYNPUF_ID', 'Year'], how='outer')\n",
    "\n",
    "# Sort the data by DESYNPUF_ID and Year\n",
    "diagnoses_long = diagnoses_long.sort_values(by=['DESYNPUF_ID', 'Year'])\n",
    "\n",
    "# Create a column to identify if there was a transition from 2 to 1\n",
    "diagnoses_long['New_CHF'] = (diagnoses_long['SP_CHF'] == 1) & (diagnoses_long.groupby('DESYNPUF_ID')['SP_CHF'].shift(1) == 2)\n",
    "\n",
    "# Group by DESYNPUF_ID to determine if new CHF was developed\n",
    "new_chf = diagnoses_long.groupby('DESYNPUF_ID')['New_CHF'].max().reset_index()\n",
    "\n",
    "# Replace boolean values with integers (1 for True, 2 for False)\n",
    "new_chf['New_CHF'] = new_chf['New_CHF'].apply(lambda x: 1 if x else 2)\n",
    "\n",
    "# Merge the New_CHF column back into the diagnoses_long DataFrame\n",
    "diagnoses_long = diagnoses_long.merge(new_chf, on='DESYNPUF_ID', suffixes=('', '_calc'))\n",
    "\n",
    "# Export the long format DataFrame to a CSV file for QA\n",
    "output_path_long = r'C:\\Users\\JoeCarhart\\OneDrive - Appriss Health LLC\\Desktop\\dx_data_long.csv'\n",
    "diagnoses_long.to_csv(output_path_long, index=False)\n",
    "\n",
    "# Display the transformed long DataFrame\n",
    "print(\"Long format Diagnoses DataFrame:\")\n",
    "print(diagnoses_long)\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use value_counts to count New CHF cases\n",
    "new_chf_counts1 = diagnoses_long.groupby('New_CHF_calc')['DESYNPUF_ID'].nunique()\n",
    "print(new_chf_counts1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diagnoses_long.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Merge diagnoses_long to df_long_final by 'DESYNPUF_ID' and 'Year'\n",
    "bene_df = pd.merge(df_long_final, diagnoses_long, on=['DESYNPUF_ID', 'Year'], how='outer')\n",
    "\n",
    "# Ensure the data is sorted by DESYNPUF_ID and Year\n",
    "bene_df = bene_df.sort_values(by=['DESYNPUF_ID', 'Year'])\n",
    "\n",
    "# Drop the 'Year' column\n",
    "bene_df = bene_df.drop(columns=['Year'])\n",
    "\n",
    "# Group by 'DESYNPUF_ID' and aggregate as required\n",
    "agg_dict = {\n",
    "    'Age': 'max',\n",
    "    'Gender': 'max',\n",
    "    'Race': 'max',\n",
    "    'State': 'max',\n",
    "    'County': 'max',\n",
    "    'annual_cost': ['sum', 'mean'],\n",
    "    'SP_ALZHDMTA': 'min',\n",
    "    'New_CHF_calc': 'min',\n",
    "    'SP_CHRNKIDN': 'min',\n",
    "    'SP_CNCR': 'min',\n",
    "    'SP_COPD': 'min',\n",
    "    'SP_DEPRESSN': 'min',\n",
    "    'SP_DIABETES': 'min',\n",
    "    'SP_ISCHMCHT': 'min',\n",
    "    'SP_OSTEOPRS': 'min',\n",
    "    'SP_RA_OA': 'min',\n",
    "    'SP_STRKETIA': 'min',\n",
    "    'BENE_ESRD_IND': 'min'\n",
    "}\n",
    "\n",
    "# Perform the aggregation\n",
    "bene_df = bene_df.groupby('DESYNPUF_ID').agg(agg_dict).reset_index()\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "bene_df.columns = [col[0] if col[1] == '' else '_'.join(col).strip() for col in bene_df.columns]\n",
    "\n",
    "# Remove suffixes '_max' and '_min' from column names\n",
    "bene_df.columns = [col.replace('_max', '').replace('_min', '') for col in bene_df.columns]\n",
    "\n",
    "# Rename the aggregated columns\n",
    "bene_df = bene_df.rename(columns={\n",
    "    'annual_cost_sum': 'total_cost',\n",
    "    'annual_cost_mean': 'average_cost'\n",
    "})\n",
    "\n",
    "# Define the path to save the CSV file for QA\n",
    "output_path = r'C:\\Users\\JoeCarhart\\OneDrive - Appriss Health LLC\\Desktop\\final_merged_data.csv'\n",
    "\n",
    "# Export the final DataFrame to a CSV file\n",
    "bene_df.to_csv(output_path, index=False)\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use value_counts to count New CHF cases\n",
    "new_chf_counts2 = bene_df.groupby('New_CHF_calc')['DESYNPUF_ID'].nunique()\n",
    "print(new_chf_counts2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bene_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Inpatient data processing**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Define base directory and file pattern\n",
    "base_dir = r'C:\\code_personal_use\\cms\\data\\ip'\n",
    "file_pattern = 'DE1_0_2008_to_2010_Inpatient_Claims_Sample_{}.csv'\n",
    "\n",
    "# Function to load and preprocess a single file\n",
    "def load_and_preprocess_file(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=['DESYNPUF_ID', 'CLM_ID', 'CLM_FROM_DT', 'CLM_THRU_DT', 'AT_PHYSN_NPI', 'CLM_UTLZTN_DAY_CNT'], dtype={'AT_PHYSN_NPI': str})\n",
    "\n",
    "    # Convert CLM_FROM_DT and CLM_THRU_DT to datetime format\n",
    "    df['CLM_FROM_DT'] = pd.to_datetime(df['CLM_FROM_DT'], format='%Y%m%d', errors='coerce')\n",
    "    df['CLM_THRU_DT'] = pd.to_datetime(df['CLM_THRU_DT'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # Drop rows with missing values in CLM_FROM_DT, AT_PHYSN_NPI, and CLM_UTLZTN_DAY_CNT\n",
    "    df = df.dropna(subset=['CLM_FROM_DT', 'AT_PHYSN_NPI', 'CLM_UTLZTN_DAY_CNT'])\n",
    "\n",
    "    # Ensure AT_PHYSN_NPI does not have scientific notation and convert it to integers\n",
    "    df['AT_PHYSN_NPI'] = df['AT_PHYSN_NPI'].apply(lambda x: int(float(x)))\n",
    "\n",
    "    # Convert CLM_UTLZTN_DAY_CNT to integers\n",
    "    df['CLM_UTLZTN_DAY_CNT'] = df['CLM_UTLZTN_DAY_CNT'].astype(int)\n",
    "\n",
    "    # Extract the year as a four-digit number and create a new column 'Year'\n",
    "    df['Year'] = df['CLM_FROM_DT'].dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "# List to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Load and preprocess all files\n",
    "for i in range(1, 21):\n",
    "    file_path = os.path.join(base_dir, file_pattern.format(i))\n",
    "    df = load_and_preprocess_file(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "ip_sample_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame and unique ID counts\n",
    "print(f'Unique IDs: {ip_sample_all[\"DESYNPUF_ID\"].nunique()}')\n",
    "\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(ip_sample_all.head())\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ip_sample_all.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Filter to include only the specific DESYNPUF_ID for EDA and QA\n",
    "# ip_sample_one = ip_sample_one[ip_sample_one['DESYNPUF_ID'] == '00016F745862898F']\n",
    "\n",
    "# Group by DESYNPUF_ID and Year, and perform the required calculations\n",
    "ip_basic_summary = ip_sample_all.groupby(['DESYNPUF_ID']).agg({\n",
    "    'CLM_ID': pd.Series.nunique,  # Count distinct CLM_ID\n",
    "    'AT_PHYSN_NPI': pd.Series.nunique,  # Count distinct AT_PHYSN_NPI\n",
    "    'CLM_UTLZTN_DAY_CNT': 'mean'  # Average CLM_UTLZTN_DAY_CNT\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns to be more descriptive\n",
    "ip_basic_summary.columns = ['DESYNPUF_ID', 'Distinct_IP_CLM_ID_Count', 'Distinct_IP_NPI_Count', 'Avg_CLM_UTLZTN_DAY_CNT']\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ip_basic_summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Outpatient data processing**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Define base directory and file pattern\n",
    "base_dir = r'C:\\code_personal_use\\cms\\data\\op'\n",
    "file_pattern = 'DE1_0_2008_to_2010_Outpatient_Claims_Sample_{}.csv'\n",
    "\n",
    "# Function to load and preprocess a single file\n",
    "def load_and_preprocess_file(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=['DESYNPUF_ID', 'CLM_ID', 'CLM_FROM_DT', 'CLM_THRU_DT', 'AT_PHYSN_NPI'], dtype={'AT_PHYSN_NPI': str})\n",
    "\n",
    "    # Convert CLM_FROM_DT and CLM_THRU_DT to datetime format\n",
    "    df['CLM_FROM_DT'] = pd.to_datetime(df['CLM_FROM_DT'], format='%Y%m%d', errors='coerce')\n",
    "    df['CLM_THRU_DT'] = pd.to_datetime(df['CLM_THRU_DT'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # Drop rows with missing values in CLM_FROM_DT and AT_PHYSN_NPI\n",
    "    df = df.dropna(subset=['CLM_FROM_DT', 'AT_PHYSN_NPI'])\n",
    "\n",
    "    # Ensure AT_PHYSN_NPI does not have scientific notation and convert it to integers\n",
    "    df['AT_PHYSN_NPI'] = df['AT_PHYSN_NPI'].apply(lambda x: int(float(x)))\n",
    "\n",
    "    # Extract the year as a four-digit number and create a new column 'Year'\n",
    "    df['Year'] = df['CLM_FROM_DT'].dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "# List to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Load and preprocess all files\n",
    "for i in range(1, 21):\n",
    "    file_path = os.path.join(base_dir, file_pattern.format(i))\n",
    "    df = load_and_preprocess_file(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "op_sample_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame and unique ID counts\n",
    "print(f'Unique IDs: {op_sample_all[\"DESYNPUF_ID\"].nunique()}')\n",
    "\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(op_sample_all.head())\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "op_sample_all.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Filter to include only the specific DESYNPUF_ID for EDA and QA\n",
    "# op_sample_one = op_sample_one[op_sample_one['DESYNPUF_ID'] == '00016F745862898F']\n",
    "\n",
    "# Group by DESYNPUF_ID and Year, and perform the required calculations\n",
    "op_basic_summary = op_sample_all.groupby(['DESYNPUF_ID']).agg({\n",
    "    'CLM_ID': pd.Series.nunique,  # Count distinct CLM_ID\n",
    "    'AT_PHYSN_NPI': pd.Series.nunique,  # Count distinct AT_PHYSN_NPI\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns to be more descriptive\n",
    "op_basic_summary.columns = ['DESYNPUF_ID', 'Distinct_OP_CLM_ID_Count', 'Distinct_OP_NPI_Count']\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "op_basic_summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A few quick checks before combining the benecificary data, inpatient data, and outpatient data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bene_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ip_basic_summary.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "op_basic_summary.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge the three data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Select the specified columns from ip_basic_summary and op_basic_summary\n",
    "ip_selected = ip_basic_summary[['DESYNPUF_ID', 'Distinct_IP_CLM_ID_Count', 'Distinct_IP_NPI_Count', 'Avg_CLM_UTLZTN_DAY_CNT']]\n",
    "op_selected = op_basic_summary[['DESYNPUF_ID', 'Distinct_OP_CLM_ID_Count', 'Distinct_OP_NPI_Count']]\n",
    "\n",
    "\n",
    "# Merge bene and ip data\n",
    "bene_ip = pd.merge(bene_df, ip_selected, on=['DESYNPUF_ID'], how='left')\n",
    "\n",
    "\n",
    "# Merge the result with outpatient data\n",
    "final_df = pd.merge(bene_ip, op_selected, on=['DESYNPUF_ID'], how='left')\n",
    "\n",
    "\n",
    "# Replace NaNs with 0\n",
    "final_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "stop_time = time.time()\n",
    "print(\"The code block took {0} seconds to run.\".format(stop_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conduct causal modeling**\n",
    "\n",
    "Need to justify choice of approaches here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's convert to traditional dummy coded values for easier end-user interpretation.\n",
    "# Columns to transform\n",
    "columns_to_transform = ['SP_ALZHDMTA', 'New_CHF_calc', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD',\n",
    "                        'SP_DEPRESSN', 'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS',\n",
    "                        'SP_RA_OA', 'SP_STRKETIA', 'BENE_ESRD_IND']\n",
    "\n",
    "# Transform the values\n",
    "final_df[columns_to_transform] = final_df[columns_to_transform].replace({2: 0, 1: 1})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define our outcome and covariates\n",
    "outcome = 'New_CHF_calc'\n",
    "covariates = [col for col in final_df.columns if col not in ['DESYNPUF_ID', 'New_CHF_calc', 'total_cost']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covariates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale covariates\n",
    "scaler = StandardScaler()\n",
    "final_df[covariates] = scaler.fit_transform(final_df[covariates])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create logistic regression model for propensity scores\n",
    "logit = LogisticRegression()\n",
    "final_df['propensity_score'] = logit.fit(final_df[covariates], final_df[outcome]).predict_proba(final_df[covariates])[:,1]\n",
    "\n",
    "# Visualize propensity scores\n",
    "sns.histplot(final_df['propensity_score'], kde=True)\n",
    "plt.title('Distribution of Propensity Scores')\n",
    "plt.show()\n",
    "\n",
    "print(final_df[['DESYNPUF_ID', 'New_CHF_calc', 'propensity_score']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Matching\n",
    "treated = final_df[final_df['New_CHF_calc'] == 1].reset_index(drop=True)\n",
    "control = final_df[final_df['New_CHF_calc'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Fit nearest neighbors model on control group\n",
    "nn = NearestNeighbors(n_neighbors=1)\n",
    "nn.fit(control[['propensity_score']])\n",
    "\n",
    "# Find nearest neighbors for treated group\n",
    "distances, indices = nn.kneighbors(treated[['propensity_score']])\n",
    "\n",
    "# Create matched pairs\n",
    "matched_control_indices = indices.flatten()\n",
    "matched_control = control.iloc[matched_control_indices].reset_index(drop=True)\n",
    "\n",
    "# Combine matched treated and control into one DataFrame\n",
    "matched_data = pd.concat([treated, matched_control], ignore_index=True)\n",
    "\n",
    "# Diagnostics: Check balance of covariates after matching\n",
    "for covariate in covariates:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.kdeplot(data=matched_data, x=covariate, hue='New_CHF_calc')\n",
    "    plt.title(f'Balance Check for {covariate}')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate average costs for treated and control groups\n",
    "average_cost_chf_pos = matched_data[matched_data['New_CHF_calc'] == 1]['total_cost'].mean()\n",
    "average_cost_chf_neg = matched_data[matched_data['New_CHF_calc'] == 0]['total_cost'].mean()\n",
    "\n",
    "print(f\"Average Cost for patients with CHF: ${average_cost_chf_pos:.2f}\")\n",
    "print(f\"Average Cost for patients without CHF: ${average_cost_chf_neg:.2f}\")\n",
    "\n",
    "# Calculate the average treatment effect on the treated (ATT)\n",
    "att = average_cost_chf_pos - average_cost_chf_neg\n",
    "print(f\"Average Cost Difference between patients with and without CHF: ${att:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare Costs\n",
    "average_cost_treated = matched_data[matched_data['New_CHF_calc'] == 1]['total_cost'].mean()\n",
    "average_cost_control = matched_data[matched_data['New_CHF_calc'] == 0]['total_cost'].mean()\n",
    "\n",
    "print(f\"Average Cost for (CHF): ${average_cost_treated:.2f}\")\n",
    "print(f\"Average Cost for (Non-CHF): ${average_cost_control:.2f}\")\n",
    "\n",
    "# Calculate the average treatment effect on the treated (ATT)\n",
    "att = average_cost_treated - average_cost_control\n",
    "print(f\"Average Intervention Effect on the CHF (AIT metric): ${att:.2f}\")\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "n_bootstraps = 1000\n",
    "att_bootstraps = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    bootstrap_sample = matched_data.sample(frac=1, replace=True)\n",
    "    average_cost_treated_boot = bootstrap_sample[bootstrap_sample['New_CHF_calc'] == 1]['total_cost'].mean()\n",
    "    average_cost_control_boot = bootstrap_sample[bootstrap_sample['New_CHF_calc'] == 0]['total_cost'].mean()\n",
    "    att_bootstraps.append(average_cost_treated_boot - average_cost_control_boot)\n",
    "\n",
    "ci_lower = np.percentile(att_bootstraps, 2.5)\n",
    "ci_upper = np.percentile(att_bootstraps, 97.5)\n",
    "\n",
    "print(f\"95% CI for ATT: (${ci_lower:.2f}, ${ci_upper:.2f})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}